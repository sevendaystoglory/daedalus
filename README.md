# daedalus
what does it take to train massive foundation models? (>1b) we are essentially distilling trillions of bytes of information roughly into ~0.001 times the storage. what gets retained and what (most) gets lost? 
