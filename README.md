# daedalus
What does it take to train massive foundation LLMs? (>1b) We are essentially distilling trillions of bytes of information roughly into ~0.001 times the storage. That has to a rather sophisticated compression algortihm to retain the important aspects of the data! What gets retained and what (most) gets lost? I've had ample experience experience fine-tuning with and without adapters. That procedure, however, operates at a stage where the model has already absorbed basic language modelling capabilities such as grammar, typography, sentence structure, etc. This project will explore the fundamentals of pretraining, and training at scale in general.

Daedalus is a silly name for such technical escapades. But this will do for now.
<img width="570" height="792" alt="image" src="https://github.com/user-attachments/assets/c3ab070c-30c4-46b6-8894-e9e27bddc323" />
